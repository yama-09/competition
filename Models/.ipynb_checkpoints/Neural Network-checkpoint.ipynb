{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ceceab",
   "metadata": {},
   "source": [
    "#### Neural Network Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b235d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x):\n",
    "        self.encoders = []\n",
    "        self.offsets = []\n",
    "        self.vocab_size = 0\n",
    "        for i in range(x.shape[1]):\n",
    "            encoder = preprocessing.LabelEncoder()\n",
    "            encoder = encoder.fit(x[:, i])\n",
    "            self.encoders.append(encoder)\n",
    "            self.offsets.append(self.vocab_size)\n",
    "            self.vocab_size += len(encoder.classes_)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        x_idx = np.zeros(x.shape)\n",
    "        for i in range(x.shape[1]):\n",
    "            x_idx[:, i] = self.encoders[i].transform(x[:, i])\n",
    "            x_idx[:, i] += self.offsets[i]\n",
    "\n",
    "        return x_idx\n",
    "\n",
    "class EmbeddingRegression():\n",
    "    def __init__(self, num_dim, cat_dim, emb_dim, vocab_size, hidden_units, batch_size, epochs, patience, log_dir):\n",
    "        # hyper parameters\n",
    "        self.num_dim = num_dim\n",
    "        self.cat_dim = cat_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        # optimizer parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.patience = patience\n",
    "\n",
    "        # logging settings\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "    def fit(self, x_num_train, x_cat_train, y_train, x_num_val, x_cat_val, y_val):\n",
    "        # convert np.array to tf.data.Dataset\n",
    "        ds_train = self.np2ds(x_num_train, x_cat_train, y_train)\n",
    "        ds_val = self.np2ds(x_num_val, x_cat_val, y_val)\n",
    "\n",
    "        # build model and compile loss, optimizer and metrics\n",
    "        self.build()\n",
    "        self.compile()\n",
    "\n",
    "        # fit model with early stopping\n",
    "        best_loss = float(\"inf\")\n",
    "        best_epoch = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_x_num, batch_x_cat, batch_y in ds_train:\n",
    "                self.train_step(batch_x_num, batch_x_cat, batch_y)\n",
    "            for batch_x_num, batch_x_cat, batch_y in ds_val:\n",
    "                self.val_step(batch_x_num, batch_x_cat, batch_y)\n",
    "\n",
    "            self.print_metrics(epoch)\n",
    "\n",
    "            if best_loss > self.val_loss.result():\n",
    "                best_loss = self.val_loss.result()\n",
    "                best_epoch = epoch\n",
    "            if epoch - best_epoch > self.patience:\n",
    "                break\n",
    "\n",
    "            self.reset_metrics()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, x_num, x_cat):\n",
    "        predictions = self.model([x_num, x_cat])\n",
    "        predictions = predictions.numpy()[:, 0]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def np2ds(self, x_num, x_cat, y):\n",
    "        ds = tf.data.Dataset.from_tensor_slices((x_num, x_cat, y))\n",
    "        ds = ds.shuffle(len(x_num))\n",
    "        ds = ds.batch(self.batch_size)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    def build(self):\n",
    "        x_num_input = tf.keras.Input(shape = (self.num_dim,), name = \"x_num\")\n",
    "        x_cat_input = tf.keras.Input(shape = (self.cat_dim,), name = \"x_cat\")\n",
    "        x_cat = tf.keras.layers.Embedding(self.vocab_size, self.emb_dim, name = \"embedding\")(x_cat_input)\n",
    "        x_cat = tf.keras.layers.Flatten(name = \"flatten\")(x_cat)\n",
    "        x = tf.keras.layers.concatenate([x_num_input, x_cat], name = \"concat\")\n",
    "        x = tf.keras.layers.BatchNormalization(name = \"hidden_norm1\")(x)\n",
    "        x = tf.keras.layers.Dense(self.hidden_units, activation = tf.keras.layers.ReLU(), name = \"hidden_dense1\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization(name = \"hidden_norm2\")(x)\n",
    "        x = tf.keras.layers.Dense(self.hidden_units, activation = tf.keras.layers.ReLU(), name = \"hidden_dense2\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization(name = \"output_norm\")(x)\n",
    "        x = tf.keras.layers.Dense(1, name = \"output_dense\")(x)\n",
    "        self.model = tf.keras.Model(inputs = [x_num_input, x_cat_input], outputs = x)\n",
    "\n",
    "    def compile(self):\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError()\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        self.train_loss = tf.keras.metrics.Mean(name = \"train_loss\")\n",
    "        self.train_accuracy = tf.keras.metrics.RootMeanSquaredError(name='train_rmse')\n",
    "        self.val_loss = tf.keras.metrics.Mean(name = \"val_loss\")\n",
    "        self.val_accuracy = tf.keras.metrics.RootMeanSquaredError(name='val_rmse')\n",
    "        \n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, x_num, x_cat, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model([x_num, x_cat])\n",
    "            loss = self.loss_object(y, predictions)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        self.train_loss(loss)\n",
    "        self.train_accuracy(y, predictions)\n",
    "\n",
    "    @tf.function\n",
    "    def val_step(self, x_num, x_cat, y):\n",
    "        predictions = self.model([x_num, x_cat])\n",
    "        loss = self.loss_object(y, predictions)\n",
    "        self.val_loss(loss)\n",
    "        self.val_accuracy(y, predictions)\n",
    "\n",
    "    def print_metrics(self, epoch):\n",
    "        template = \"Epoch {}, tr_RMSE: {}, va_RMSE: {}\"\n",
    "        print(template.format(\n",
    "            epoch + 1,\n",
    "            self.train_accuracy.result(),\n",
    "            self.val_accuracy.result()\n",
    "        ))\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        self.train_loss.reset_states()\n",
    "        self.train_accuracy.reset_states()\n",
    "        self.val_loss.reset_states()\n",
    "        self.val_accuracy.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6ba04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_csv(\"./data/hu_train.csv\")\n",
    "test = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "\n",
    "traintest = pd.concat([train, test], ignore_index = True, sort = False)\n",
    "\n",
    "\n",
    "# list numeric and categorical columns\n",
    "num_cols = ['year','month','day','lat', 'lon', 'co_cnt', 'co_min', 'co_mid', 'co_max',\n",
    "   'co_var', 'o3_cnt', 'o3_min', 'o3_mid', 'o3_max', 'o3_var', 'so2_cnt',\n",
    "   'so2_min', 'so2_mid', 'so2_max', 'so2_var', 'no2_cnt', 'no2_min',\n",
    "   'no2_mid', 'no2_max', 'no2_var', 'temperature_cnt', 'temperature_min',\n",
    "   'temperature_mid', 'temperature_max', 'temperature_var', 'humidity_cnt',\n",
    "   'humidity_min', 'humidity_mid', 'humidity_max', 'humidity_var',\n",
    "   'pressure_cnt', 'pressure_min', 'pressure_mid', 'pressure_max',\n",
    "   'pressure_var', 'ws_cnt', 'ws_min', 'ws_mid', 'ws_max', 'ws_var',\n",
    "   'dew_cnt', 'dew_min', 'dew_mid', 'dew_max', 'dew_var']\n",
    "cat_cols = [\"Country\",\"City\"]\n",
    "\n",
    "# split data to numeric, categorical and target column(s)\n",
    "x_num_trainval = train[num_cols].values\n",
    "x_cat_trainval = train[cat_cols].values\n",
    "y_trainval = train[\"pm25_mid\"].values\n",
    "x_num_test = test[num_cols].values\n",
    "x_cat_test = test[cat_cols].values\n",
    "\n",
    "# tokenize categorical columns\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer = tokenizer.fit(traintest[cat_cols].values)\n",
    "x_cat_trainval = tokenizer.transform(x_cat_trainval)\n",
    "x_cat_test = tokenizer.transform(x_cat_test)\n",
    "\n",
    "# train\n",
    "n = 1\n",
    "models = []\n",
    "for i in range(n):\n",
    "    x_num_train, x_num_val, x_cat_train, x_cat_val, y_train, y_val = train_test_split(x_num_trainval, x_cat_trainval, y_trainval, test_size = 0.2, random_state = i)\n",
    "\n",
    "    model = EmbeddingRegression(\n",
    "        num_dim = x_num_train.shape[1],\n",
    "        cat_dim = x_cat_train.shape[1],\n",
    "        emb_dim = 2,\n",
    "        vocab_size = tokenizer.vocab_size,\n",
    "        hidden_units = 128,\n",
    "        batch_size = 128,\n",
    "        epochs = 1000,\n",
    "        patience = 50,\n",
    "        log_dir = Path(\"logs\", \"{}-{:02d}\".format(timestamp, i))\n",
    "    )\n",
    "    model = model.fit(x_num_train, x_cat_train, y_train, x_num_val, x_cat_val, y_val)\n",
    "    models.append(model)\n",
    "\n",
    "# predict\n",
    "y_test = np.zeros(len(test))\n",
    "for model in models:\n",
    "    y_test += model.predict(x_num_test, x_cat_test)\n",
    "y_test /= len(models)\n",
    "\n",
    "# export predictions\n",
    "submit = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"y\": y_test\n",
    "})\n",
    "\n",
    "for i in range(len(submit)): \n",
    "    if submit[\"y\"][i] < 0:\n",
    "        submit[\"y\"][i] = 0\n",
    "        \n",
    "submit.to_csv(\"NN_submit.csv\", header = None, index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
